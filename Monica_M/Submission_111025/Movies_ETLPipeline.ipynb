{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c75cbe8-afe6-4c45-b910-19856c69daa3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ETL Pipeline for Move Analystics \n",
    "## A modular, production grade data pipeline built in Python using pandas\n",
    "\n",
    "This notebook showcases a scalable Extract, Transform, Load (ETL) pipeline designed for analystics workflows in the entertainmenrt domain. It ingests raw CSV/TSC datasets from IMDb, The Numbers and MovieLens, performs column-level cleaning, profit and ROI enrichment and produces analysis ready DataFrames and CSV exports.\n",
    "\n",
    "I have chose this code structure to reflect best practices in data engineering, including:\n",
    "- Clean OOP abstraction (DataExtractor, DataTransformer, DataLoader, ETLPipeline)\n",
    "- Strict typing & docstring standards\n",
    "- Idempotent, testable transformations\n",
    "- Modular design allowing quick substitution of data sources (eg. S3, SQL, API)\n",
    "- Local and remote execution compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c592d40e-e009-4131-87b3-a75de92b8e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this project, each stage of the pipeline - Extract, Transform, Load and Orchestrate is implemented as a python class. The classes are as follows:\n",
    "- `DataExtractor`\n",
    "- `DataTransformer`\n",
    "- `DataLoader`\n",
    "- `ETLPipeline`\n",
    "\n",
    "A class is a blueprint for crearing objects that bundle data (attributes) and behaviour (methods) together. It allows us to model real-world entities as self-contained, reusable components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8e4d227-bd49-4392-b4ab-64fd44ac8035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataExtractor\n",
    "The aim of the DataExtractor class is to extract the tablular data from the remote URL's (.csv, .tsv), use robust pandas parsing ad validates file types. It then returns a dictionary of DataFrames keyed by table name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33287ffb-8770-4214-946f-536fff899987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "clean_folder='path/clean_folder'\n",
    "raw_folder = 'path/raw_folder'\n",
    "transformed_folder = 'path/transformed_folder'\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class DataExtractor:\n",
    "    \"\"\"\n",
    "    DataExtractor is responsible for extracting data from given URLs (CSV/TSV) and loading them into pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "        tables (dict): A dictionary mapping URLs to table names.\n",
    "\n",
    "    Methods:\n",
    "        extract(): Reads each file from the URLs and returns a dictionary of DataFrames keyed by table name.\n",
    "    \"\"\"\n",
    "    def __init__(self, tables: dict):\n",
    "        \"\"\"\n",
    "        Initializes the DataExtractor with a dictionary of table URLs and names.\n",
    "\n",
    "        Args:\n",
    "            tables (dict): A dictionary mapping URLs to table names.\n",
    "        \"\"\"\n",
    "        self.tables = tables\n",
    "    \n",
    "    def extract(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataExtractor with a dictionary of table URLs and names.\n",
    "\n",
    "        Args:\n",
    "            tables (dict): A dictionary mapping URLs to table names.\n",
    "            \"\"\"\n",
    "        data = {}\n",
    "        for url, table_name in self.tables.items():\n",
    "            if url.endswith('.csv'):\n",
    "                data[table_name] = pd.read_csv(url, engine=\"python\")\n",
    "            elif url.endswith('.tsv'):\n",
    "                data[table_name] = pd.read_csv(url, delimiter='\\t', engine=\"python\")\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae452ce8-2ff0-4537-a500-70659ca64284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Transformer\n",
    "The aim of the DataTransformer is to clean the numberic columns in our dataframes to remove thw currency symbols and commas. It also coerces types safely with `errors=\"coerce\"` so that for example in the event that we are converting data like strings to numbers, any invalid values are turned into NaN instead of causing an error. In addition it supports configurable per-table cleaning specifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "206aa367-61cf-42ff-96b6-0ec7d58f61b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataTransformer:\n",
    "    \"\"\"\n",
    "    This module provides an ETL pipeline for extracting, cleaning, and saving movie-related datasets from various CSV/TSV URLs.\n",
    "    Classes:\n",
    "        - DataExtractor: Downloads and loads data into pandas DataFrames.\n",
    "        - DataTransformer: Cleans specified columns in the DataFrames.\n",
    "        - DataLoader: Saves DataFrames to CSV files in specified folders.\n",
    "        - ETLPipeline: Orchestrates the ETL process using the above components.\n",
    "    \"\"\"\n",
    "    def __init__(self, columns_to_clean: dict):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformer with a dictionary of columns to clean for each table.\n",
    "        \"\"\"\n",
    "        self.columns_to_clean = columns_to_clean\n",
    "\n",
    "    def clean(self, data):\n",
    "        \"\"\"\n",
    "        Cleans specified columns across the provided tables.\n",
    "\n",
    "        For each configured column, it removes '$' and ',' from the column values and converts them to numeric types.\n",
    "        (invalid values are set to NaN)\n",
    "        \n",
    "        Args:\n",
    "            data (dict): A dictionary of DataFrames keyed by table name.\n",
    "        Returns:\n",
    "            dict: The cleaned DataFrames.\n",
    "        \"\"\"\n",
    "        for table_name, columns in self.columns_to_clean.items():\n",
    "            for col in columns:\n",
    "                if data[table_name][col].dtype =='object':\n",
    "                    data[table_name][col] = (\n",
    "                        data[table_name][col]\n",
    "                        .str.replace('$', '', regex=False)\n",
    "                        .str.replace(',', '', regex=False)\n",
    "                    )\n",
    "                    data[table_name][col] = pd.to_numeric(data[table_name][col], errors='coerce')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87f4347a-fbec-4265-a277-c0f55bab865b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DataLoader\n",
    "This aims to write the Datafranes to CSV in structured folders, ensures the destination folders exist. This can be extenxed to upload to Amazon S3, Google Cloud Storage or SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7efcc41-6bc8-4258-86de-e4d00038185f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    DataLoader is responsible for saving DataFrames to CSV files in a specified destination folder.\n",
    "\n",
    "    Args:\n",
    "        destination_folder (str): The folder path where the CSV files will be saved.\n",
    "\n",
    "    Methods:\n",
    "        load(data): Saves each DataFrame in the data dictionary to a CSV file in the destination folder.\n",
    "    \"\"\"\n",
    "    def __init__(self, destination_folder: str):\n",
    "        \"\"\"\n",
    "        Initializes the DataLoader with the specified destination folder.\n",
    "\n",
    "        Args:\n",
    "            destination_folder (str): The folder path where the CSV files will be saved.\n",
    "        \"\"\"\n",
    "        self.destination_folder = destination_folder\n",
    "\n",
    "    def load(self, data):\n",
    "        \"\"\"\n",
    "        DataLoader is responsible for saving DataFrames to CSV files in a specified destination folder.\n",
    "\n",
    "        Args:\n",
    "            destination_folder (str): The folder path where the CSV files will be saved.\n",
    "\n",
    "        Methods:\n",
    "            load(data): Saves each DataFrame in the data dictionary to a CSV file in the destination folder.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.destination_folder, exist_ok=True)\n",
    "        for table_name, df in data.items():\n",
    "            path = os.path.join(self.destination_folder, f\"{table_name}.csv\")\n",
    "            df.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a26486b-01f1-4458-918f-1e6500bae2fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ETLPipeline\n",
    "The aim of the ETLPipeline class is to orchestrate all of the stages that have been set out in the previous classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa97a460-705b-4a0f-9f2a-7ce688d65195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class ETLPipeline:\n",
    "    \"\"\"\n",
    "    ETLPipeline orchestrates the ETL process: extraction, cleaning, and saving of movie-related datasets.\n",
    "\n",
    "    Args:\n",
    "        tables (dict): Mapping of URLs to table names.\n",
    "        columns_to_clean (dict): Columns to clean for each table.\n",
    "        folders (tuple): Tuple of (raw_folder, clean_folder, transformed_folder) paths.\n",
    "\n",
    "    Methods:\n",
    "        run(): Executes the ETL pipeline steps.\n",
    "    \"\"\"\n",
    "    def __init__(self, tables, columns_to_clean, folders):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline and its components.\n",
    "\n",
    "        Args:\n",
    "            tables (dict): Mapping of URLs to table names.\n",
    "            columns_to_clean (dict): Columns to clean for each table.\n",
    "            folders (tuple): Tuple of (raw_folder, clean_folder, transformed_folder) paths.\n",
    "        \"\"\"\n",
    "        self.extractor = DataExtractor(tables)\n",
    "        self.transformer = DataTransformer(columns_to_clean)\n",
    "        self.raw_folder, self.clean_folder, self.transformed_folder = folders\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute extraction and saving raw  data, cleaning, and saving cleaned data, and transforming and saving transformed data.\n",
    "        \"\"\"\n",
    "        print(\"Extracting data...\")\n",
    "        data = self.extractor.extract()\n",
    "\n",
    "        print(\"Saving raw data...\")\n",
    "        DataLoader(self.raw_folder).load(data)\n",
    "\n",
    "        print(\"Cleaning data...\")\n",
    "        cleaned = self.transformer.clean(data)\n",
    "        DataLoader(self.clean_folder).load(cleaned)\n",
    "\n",
    "        print(\"Transforming data...\")\n",
    "        # your transform_data() logic can go here\n",
    "        transformed = cleaned\n",
    "        DataLoader(self.transformed_folder).load(transformed)\n",
    "\n",
    "        print(\"ETL complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6197f7d-c223-4ab2-bd53-02fc7969a1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run the ETL Pipline\n",
    "if __name__ == \"__main__\":\n",
    "    TABLES = {\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/MovieLens_movies.csv\": \"movies_Id\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/IMDb%20BoxOfficeMojo%20-%20Brands%20(US%20%26%20Canada).tsv\": \"brands_US_and_Canada\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/IMDb%20BoxOfficeMojo%20-%20Brand_%20Marvel%20Comics.tsv\": \"brand_marvel_comics\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/The%20Numbers%20-%20Domestic%20Box%20Office%20Daily%20-%20The%20Avengers.tsv\": \"Domestic_Box_Office_Daily_The_Avengers\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/The%20Numbers%20-%20Domestic%20Box%20Office%20-%20Franchises.tsv\": \"Domestic_Box_Office_Franchises\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/The%20Numbers%20-%20Domestic%20Box%20Office%20-%20Franchises%20-%20Marvel%20Cinematic%20Universe.tsv\": \"Domestic_Box_Office_Franchises_Marvel_Cinematic\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/World%20Wide%20Box%20Office%20All%20Time%20Top%201000.tsv\": \"World_Wide_Box_Office_All_Time_Top_1000\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/IMDb%20BoxOfficeMojo%20-%20Franchises%20(US%20%26%20Canada).tsv\": \"Franchises_us_and_Canada\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/IMDb%20BoxOfficeMojo%20-%20Franchise_%20top20.tsv\": \"top_20_for_each_Franchise\",\n",
    "    \"https://raw.githubusercontent.com/mansik95/IMDB-Analysis/master/Data/MovieLens_tags.csv\": \"tags\"\n",
    "    }\n",
    "\n",
    "    COLUMNS_TO_CLEAN = {\n",
    "    'Domestic_Box_Office_Franchises': ['Domestic_Box_Office', 'Infl_Adj_Dom_Box_Office', 'Worldwide_Box_Office'],\n",
    "    'Domestic_Box_Office_Franchises_Marvel_Cinematic': ['Production_Budget', 'Opening_Weekend', 'Domestic_Box_Office', 'Worldwide_Box_Office'],\n",
    "    'top_20_for_each_Franchise': ['Lifetime_Gross','Opening_Gross','Max_Theaters'],\n",
    "    'brand_marvel_comics': ['Lifetime_Gross','Max_Theaters','Opening_Gross','Open_Theaters'],\n",
    "    'Franchises_us_and_Canada': ['Lifetime_Gross','Total_Revenue'],\n",
    "    'brands_US_and_Canada': ['Total_Gross','Lifetime_Gross'],\n",
    "    'Domestic_Box_Office_Daily_The_Avengers': ['Gross','Theaters','Per Theater','Total Gross'],\n",
    "    }\n",
    "\n",
    "    FOLDERS = (clean_folder,\n",
    "               raw_folder,\n",
    "               transformed_folder)\n",
    "    \n",
    "\n",
    "pipeline = ETLPipeline(TABLES, COLUMNS_TO_CLEAN, FOLDERS)\n",
    "pipeline.run()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Movies_ETLPipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
